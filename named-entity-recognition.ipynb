{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9da70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets scipy scikit-learn ipywidgets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bca660-2a42-467c-8db8-cd197a2be613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aef1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\"\n",
    "model_checkpoint = \"klue/roberta-base\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac95666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset klue (/root/.cache/huggingface/datasets/klue/ner/1.0.0/55ff8f92b7a4b9842be6514ce0b4b5295b46d5e493f8bb5760da4be717018f90)\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"klue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0cf6aef-5ece-42d4-a960-3e5b40890453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ner_tags', 'sentence', 'tokens'],\n",
       "        num_rows: 21008\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ner_tags', 'sentence', 'tokens'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8bd6c6-0ada-4d99-9285-0161e2b3778e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ner_tags': [12,\n",
       "  12,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  8,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12],\n",
       " 'sentence': '특히 <영동고속도로:LC> <강릉:LC> 방향 <문막휴게소:LC>에서 <만종분기점:LC>까지 <5㎞:QT> 구간에는 승용차 전용 임시 갓길차로제를 운영하기로 했다.',\n",
       " 'tokens': ['특',\n",
       "  '히',\n",
       "  ' ',\n",
       "  '영',\n",
       "  '동',\n",
       "  '고',\n",
       "  '속',\n",
       "  '도',\n",
       "  '로',\n",
       "  ' ',\n",
       "  '강',\n",
       "  '릉',\n",
       "  ' ',\n",
       "  '방',\n",
       "  '향',\n",
       "  ' ',\n",
       "  '문',\n",
       "  '막',\n",
       "  '휴',\n",
       "  '게',\n",
       "  '소',\n",
       "  '에',\n",
       "  '서',\n",
       "  ' ',\n",
       "  '만',\n",
       "  '종',\n",
       "  '분',\n",
       "  '기',\n",
       "  '점',\n",
       "  '까',\n",
       "  '지',\n",
       "  ' ',\n",
       "  '5',\n",
       "  '㎞',\n",
       "  ' ',\n",
       "  '구',\n",
       "  '간',\n",
       "  '에',\n",
       "  '는',\n",
       "  ' ',\n",
       "  '승',\n",
       "  '용',\n",
       "  '차',\n",
       "  ' ',\n",
       "  '전',\n",
       "  '용',\n",
       "  ' ',\n",
       "  '임',\n",
       "  '시',\n",
       "  ' ',\n",
       "  '갓',\n",
       "  '길',\n",
       "  '차',\n",
       "  '로',\n",
       "  '제',\n",
       "  '를',\n",
       "  ' ',\n",
       "  '운',\n",
       "  '영',\n",
       "  '하',\n",
       "  '기',\n",
       "  '로',\n",
       "  ' ',\n",
       "  '했',\n",
       "  '다',\n",
       "  '.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860fa66b-0831-45dc-a195-de31bf870ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=13, names=['B-DT', 'I-DT', 'B-LC', 'I-LC', 'B-OG', 'I-OG', 'B-PS', 'I-PS', 'B-QT', 'I-QT', 'B-TI', 'I-TI', 'O'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features[f\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f303bd4-5c9b-42ec-83ca-194e3662545a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-DT',\n",
       " 'I-DT',\n",
       " 'B-LC',\n",
       " 'I-LC',\n",
       " 'B-OG',\n",
       " 'I-OG',\n",
       " 'B-PS',\n",
       " 'I-PS',\n",
       " 'B-QT',\n",
       " 'I-QT',\n",
       " 'B-TI',\n",
       " 'I-TI',\n",
       " 'O']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225341b4-9d74-47aa-b024-5b4d5b570232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6826a861-3234-4a6f-8aa9-e7da65ac1329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LC, I-LC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>뭐 하나 빠지지 않는 드라마ㅋㅋ 러브라인도 &lt;한국:LC&gt;처럼 대놓고 스토리고 뭐고 주인공들 엮기에 급급하지 않고 아슬아슬(?)하게 해서 더 좋았다</td>\n",
       "      <td>[뭐,  , 하, 나,  , 빠, 지, 지,  , 않, 는,  , 드, 라, 마, ㅋ, ㅋ,  , 러, 브, 라, 인, 도,  , 한, 국, 처, 럼,  , 대, 놓, 고,  , 스, 토, 리, 고,  , 뭐, 고,  , 주, 인, 공, 들,  , 엮, 기, 에,  , 급, 급, 하, 지,  , 않, 고,  , 아, 슬, 아, 슬, (, ?, ), 하, 게,  , 해, 서,  , 더,  , 좋, 았, 다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, B-PS, I-PS, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PS, I-PS, I-PS, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>평소 &lt;코난:PS&gt; 자주 보는데 완전 재미있었고, &lt;유명한:PS&gt;이 대박이었다!!!!!!!!</td>\n",
       "      <td>[평, 소,  , 코, 난,  , 자, 주,  , 보, 는, 데,  , 완, 전,  , 재, 미, 있, 었, 고, ,,  , 유, 명, 한, 이,  , 대, 박, 이, 었, 다, !, !, !, !, !, !, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-TI, I-TI, I-TI, O, O, O, O, O, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O]</td>\n",
       "      <td>그냥 고전적으로 &lt;2D:QT&gt; 로 쭉 가는게 맞는거 같고 무슨 애니매이션 영화가 &lt;30분:TI&gt;이냐 ㅋㅋ어이없어서 &lt;0점:QT&gt; 주고 싶은데 &lt;1점:QT&gt;주고 간다.</td>\n",
       "      <td>[그, 냥,  , 고, 전, 적, 으, 로,  , 2, D,  , 로,  , 쭉,  , 가, 는, 게,  , 맞, 는, 거,  , 같, 고,  , 무, 슨,  , 애, 니, 매, 이, 션,  , 영, 화, 가,  , 3, 0, 분, 이, 냐,  , ㅋ, ㅋ, 어, 이, 없, 어, 서,  , 0, 점,  , 주, 고,  , 싶, 은, 데,  , 1, 점, 주, 고,  , 간, 다, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, B-OG, I-OG, I-OG, O, O, B-OG, I-OG, I-OG, I-OG, I-OG, I-OG, I-OG, I-OG, I-OG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O, B-QT, I-QT, I-QT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O, O, O, B-QT, I-QT, I-QT, O, O, O, O, ...]</td>\n",
       "      <td>한편 &lt;복지부:OG&gt;는 &lt;에볼라 긴급구호대:OG&gt;에 소속될 민간 의료인력으로 예비인력을 포함해 의사 &lt;8명:QT&gt;, 간호사 &lt;12명:QT&gt;을 선발했으며, 이 가운데 의사 &lt;6명:QT&gt;, 간호사 &lt;9명:QT&gt;이 군 의료진 &lt;15명:QT&gt;과 함께 &lt;세 차례:QT&gt;로 나뉘어 &lt;시에라리온:LC&gt;에 파견될 예정이다.</td>\n",
       "      <td>[한, 편,  , 복, 지, 부, 는,  , 에, 볼, 라,  , 긴, 급, 구, 호, 대, 에,  , 소, 속, 될,  , 민, 간,  , 의, 료, 인, 력, 으, 로,  , 예, 비, 인, 력, 을,  , 포, 함, 해,  , 의, 사,  , 8, 명, ,,  , 간, 호, 사,  , 1, 2, 명, 을,  , 선, 발, 했, 으, 며, ,,  , 이,  , 가, 운, 데,  , 의, 사,  , 6, 명, ,,  , 간, 호, 사,  , 9, 명, 이,  , 군,  , 의, 료, 진,  , 1, 5, 명, 과,  , 함, 께, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LC, I-LC, I-LC, I-LC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>k팝써바이벌은 세계인들이 주목한 행사로써 &lt;대한민국:LC&gt; 위상이 올라가는 계기가 됐다</td>\n",
       "      <td>[k, 팝, 써, 바, 이, 벌, 은,  , 세, 계, 인, 들, 이,  , 주, 목, 한,  , 행, 사, 로, 써,  , 대, 한, 민, 국,  , 위, 상, 이,  , 올, 라, 가, 는,  , 계, 기, 가,  , 됐, 다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-QT, I-QT, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>이런 영화를 위한 장르명을 새로 &lt;하나:QT&gt; 지어줘야한다.</td>\n",
       "      <td>[이, 런,  , 영, 화, 를,  , 위, 한,  , 장, 르, 명, 을,  , 새, 로,  , 하, 나,  , 지, 어, 줘, 야, 한, 다, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[B-PS, I-PS, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>&lt;보영:PS&gt;은 아직 신인때라 그런지 조금 미성숙한 연기임..그리고 영화 내용이나 연출도 많이 떨어지내요...</td>\n",
       "      <td>[보, 영, 은,  , 아, 직,  , 신, 인, 때, 라,  , 그, 런, 지,  , 조, 금,  , 미, 성, 숙, 한,  , 연, 기, 임, ., ., 그, 리, 고,  , 영, 화,  , 내, 용, 이, 나,  , 연, 출, 도,  , 많, 이,  , 떨, 어, 지, 내, 요, ., ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-PS, I-PS, O, B-PS, I-PS, O, O, O, O, O, O, B-PS, I-PS, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-DT, I-DT, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>전설적인 록그룹 &lt;유투:PS&gt;(&lt;U2:PS&gt;)의 보컬 &lt;보노:PS&gt;가 자전거 사고 후유증으로 다시는 기타를 연주할 수 없을지도 모른다고 &lt;1일:DT&gt;(현지시간) 밝혔다.</td>\n",
       "      <td>[전, 설, 적, 인,  , 록, 그, 룹,  , 유, 투, (, U, 2, ), 의,  , 보, 컬,  , 보, 노, 가,  , 자, 전, 거,  , 사, 고,  , 후, 유, 증, 으, 로,  , 다, 시, 는,  , 기, 타, 를,  , 연, 주, 할,  , 수,  , 없, 을, 지, 도,  , 모, 른, 다, 고,  , 1, 일, (, 현, 지, 시, 간, ),  , 밝, 혔, 다, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[B-DT, I-DT, I-DT, I-DT, I-DT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>&lt;올해 9월:DT&gt;개봉작 뭔러너? 예고보는데 이 영화 생각나길래..</td>\n",
       "      <td>[올, 해,  , 9, 월, 개, 봉, 작,  , 뭔, 러, 너, ?,  , 예, 고, 보, 는, 데,  , 이,  , 영, 화,  , 생, 각, 나, 길, 래, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[O, O, O, B-PS, I-PS, I-PS, O, O, O, O, B-OG, I-OG, I-OG, O, O, O, O, O, O, O, O, O, B-PS, I-PS, I-PS, O, O, O, O, O, O, O, O, B-PS, I-PS, I-PS, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-DT, I-DT, I-DT, O, B-OG, I-OG, I-OG, I-OG, I-OG, O, O, O, O, O, O, O]</td>\n",
       "      <td>배우 &lt;이유리:PS&gt; 씨가 &lt;MBC:OG&gt; 드라마 '내딸 &lt;금사월:PS&gt;' 마지막회에 &lt;연민정:PS&gt; 역으로 깜짝 출연할 예정이었지만 다른 스케줄 때문에 무산됐다고 &lt;29일:DT&gt; &lt;마이데일리:OG&gt;가 보도했다.</td>\n",
       "      <td>[배, 우,  , 이, 유, 리,  , 씨, 가,  , M, B, C,  , 드, 라, 마,  , ', 내, 딸,  , 금, 사, 월, ',  , 마, 지, 막, 회, 에,  , 연, 민, 정,  , 역, 으, 로,  , 깜, 짝,  , 출, 연, 할,  , 예, 정, 이, 었, 지, 만,  , 다, 른,  , 스, 케, 줄,  , 때, 문, 에,  , 무, 산, 됐, 다, 고,  , 2, 9, 일,  , 마, 이, 데, 일, 리, 가,  , 보, 도, 했, 다, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408422ba-70e7-456d-8599-4ff37b30d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1434282-eaff-49fc-aee4-5058d518d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 13472, 10211, 2036, 16, 25419, 11376, 23828, 17219, 30062, 9963, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38673085-1f37-47dd-a5f0-c679095310eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 13472, 10211, 2036, 16, 25419, 11376, 23828, 17219, 30062, 9963, 12405, 2010, 4412, 20641, 2036, 90, 13135, 2041, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd7e1b0-86a2-4b0d-995a-c89ead707c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['팀', ' ', '조', '연', '들', '의', ' ', '탁', '월', '한', ' ', '연', '기', '와', ' ', '홍', '금', '보', '의', ' ', '녹', '슬', '지', '않', '은', ' ', '관', '록', '의', ' ', '무', '술', '!']\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][4]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9319131d-3258-4cdd-8836-64f8aa2e8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '팀', '조', '연', '들', '의', '탁', '월', '한', '연', '기', '와', '홍', '금', '보', '의', '녹', '슬', '지', '않', '은', '관', '록', '의', '무', '술', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92df4e26-d5a0-4015-b2f9-db05a38f4637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5b6a26-5726-4b22-9bef-622385201bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 32, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfa19206-f503-428a-be29-7e3e7e4f915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7d5dda-74c8-4ead-bc79-65fbfef671e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccf24722-cd3f-46df-9c00-d100db2d9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "813fc412-1c5a-4978-8ff0-3c4eee04809e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1813, 1969, 1437, 856, 594, 1283, 848, 991, 553, 1026, 1129, 1904, 1091, 1037, 1956, 578, 1282, 1421, 1258, 1038, 1558, 1175, 645, 1540, 653, 1583, 25, 207, 615, 545, 1421, 793, 1324, 1468, 1632, 1537, 1468, 1510, 1325, 551, 647, 1632, 991, 1545, 1022, 1471, 1437, 1889, 645, 991, 1902, 809, 18, 3], [2, 1891, 617, 842, 1258, 1885, 1023, 1498, 743, 1088, 727, 1187, 1891, 1518, 1873, 1511, 801, 809, 18, 3], [2, 1889, 1583, 1038, 1504, 1437, 1933, 1421, 793, 548, 1969, 1156, 25, 558, 1022, 1564, 594, 1335, 809, 3], [2, 1504, 1141, 1038, 723, 1498, 1793, 823, 991, 1238, 1268, 1093, 1235, 570, 1263, 1173, 1091, 823, 1890, 1250, 645, 1517, 812, 23, 645, 793, 1388, 1495, 991, 25, 558, 1479, 545, 1936, 1397, 1498, 1857, 1689, 578, 860, 809, 18, 3], [2, 1823, 1552, 1431, 882, 1503, 1762, 1479, 1891, 1431, 645, 1453, 1932, 641, 1160, 1503, 767, 1319, 1583, 1380, 1497, 606, 992, 1503, 1088, 1299, 5, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 12, 12, 2, 3, 3, 3, 3, 3, 2, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100], [-100, 8, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100], [-100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 8, 9, 12, 12, 12, 12, 12, -100], [-100, 12, 12, 12, 12, 12, 12, 12, 12, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 9, 12, 12, 12, 12, 0, 1, 1, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100], [-100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aca0a36-6d75-4c7c-8330-e7512fa35d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c9e24a27414a07b1c9264dbc3099a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6ed6e9e77b42c4bf21048fb50679dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1acc170-8ea3-4181-97b9-d13767b35b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94a3b3e8-bf03-4a08-9dd1-160752ef3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c88f6e4b-ec71-41a1-b492-c433bc9fa0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9af32aff-9986-49c9-a164-cf69a05d2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f0c816c-6bc6-4a5a-b82a-9444eb3faa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PS': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b81818b-1bbb-42d0-8e71-341e2e0d2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2499a36-cc51-44df-b460-8fa05bcaebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c81aabd0-5e70-449b-b852-decf522089fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  67/3939 00:13 < 13:05, 4.93 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 active_labels = torch.where(\n\u001b[0;32m-> 1369\u001b[0;31m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m                 )\n\u001b[1;32m   1371\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e581624-3b5b-4006-90b9-329704da14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfb239-5828-4877-af84-286760b76711",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaae742-6908-45cb-96bd-5a5238104207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
