{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9da70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb\n",
    "\n",
    "Fine-tuning BERT for named-entity recognition\n",
    "In this notebook, we are going to use BertForTokenClassification which is included in the Transformers library by HuggingFace. This model has BERT as its base architecture, with a token classification head on top, allowing it to make predictions at the token level, rather than the sequence level. Named entity recognition is typically treated as a token classification problem, so that's what we are going to use it for.\n",
    "\n",
    "This tutorial uses the idea of transfer learning, i.e. first pretraining a large neural network in an unsupervised way, and then fine-tuning that neural network on a task of interest. In this case, BERT is a neural network pretrained on 2 tasks: masked language modeling and next sentence prediction. Now, we are going to fine-tune this network on a NER dataset. Fine-tuning is supervised learning, so this means we will need a labeled dataset.\n",
    "\n",
    "If you want to know more about BERT, I suggest the following resources:\n",
    "\n",
    "the original paper\n",
    "Jay Allamar's blog post as well as his tutorial\n",
    "Chris Mccormick's Youtube channel\n",
    "Abbishek Kumar Mishra's Youtube channel\n",
    "The following notebook largely follows the same structure as the tutorials by Abhishek Kumar Mishra. For his tutorials on the Transformers library, see his Github repository.\n",
    "\n",
    "NOTE: this notebook assumes basic knowledge about deep learning, BERT, and native PyTorch. If you want to learn more Python, deep learning and PyTorch, I highly recommend cs231n by Stanford University and the FastAI course by Jeremy Howard et al. Both are freely available on the web.\n",
    "\n",
    "Now, let's move on to the real stuff!\n",
    "\n",
    "Importing Python Libraries and preparing the environment\n",
    "This notebook assumes that you have the following libraries installed:\n",
    "\n",
    "pandas\n",
    "numpy\n",
    "sklearn\n",
    "pytorch\n",
    "transformers\n",
    "seqeval\n",
    "As we are running this in Google Colab, the only libraries we need to additionally install are transformers and seqeval (GPU version):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers seqeval[gpu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Downloading and preprocessing the data\n",
    "Named entity recognition (NER) uses a specific annotation scheme, which is defined (at least for European languages) at the word level. An annotation scheme that is widely used is called IOB-tagging), which stands for Inside-Outside-Beginning. Each tag indicates whether the corresponding word is inside, outside or at the beginning of a specific named entity. The reason this is used is because named entities usually comprise more than 1 word.\n",
    "\n",
    "Let's have a look at an example. If you have a sentence like \"Barack Obama was born in Hawa√Ø\", then the corresponding tags would be [B-PERS, I-PERS, O, O, O, B-GEO]. B-PERS means that the word \"Barack\" is the beginning of a person, I-PERS means that the word \"Obama\" is inside a person, \"O\" means that the word \"was\" is outside a named entity, and so on. So one typically has as many tags as there are words in a sentence.\n",
    "\n",
    "So if you want to train a deep learning model for NER, it requires that you have your data in this IOB format (or similar formats such as BILOU). There exist many annotation tools which let you create these kind of annotations automatically (such as Spacy's Prodigy, Tagtog or Doccano). You can also use Spacy's biluo_tags_from_offsets function to convert annotations at the character level to IOB format.\n",
    "\n",
    "Here, we will use a NER dataset from Kaggle that is already in IOB format. One has to go to this web page, download the dataset, unzip it, and upload the csv file to this notebook. Let's print out the first few rows of this csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:5] not in tags.keys():\n",
    "            tags[tag[2:5]] = count\n",
    "        else:\n",
    "            tags[tag[2:5]] += count\n",
    "    continue\n",
    "\n",
    "print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
